from __future__ import annotations
from typing import TYPE_CHECKING, cast
import json
from os import path as os_path
import sys
import logging

from blueshift.calendar.trading_calendar import TradingCalendar
from blueshift.lib.common.ctx_mgrs import TimeoutRLock
from blueshift.lib.common.constants import CCY, Currency
from blueshift.lib.common.platform import ensure_directory, get_exception
from blueshift.lib.common.enums import BlotterType, AlgoMode, AlgoCallBack
from blueshift.lib.exceptions import (
        BlueshiftException, ExceptionHandling, DataWriteException,
        InitializationError, TerminationError, AlgoStartError)
from blueshift.interfaces.assets._assets import MarketData
from blueshift.lib.serialize.json import convert_nan, BlueshiftJSONEncoder
from blueshift.config import (
        blueshift_saved_performance_path, blueshift_run_path, LOCK_TIMEOUT)
from blueshift.interfaces.trading.blotter import IBlotter, register_blotter
from blueshift.interfaces.assets.assets import IAssetFinder
from blueshift.interfaces.trading.broker import IBacktestBroker
from blueshift.interfaces.data.data_portal import DataPortal
from blueshift.providers.trading.broker.paper import PaperBroker

from ..trackers._perfs import PerformanceTracker
from ..analytics.pertrade import (
        create_txns_frame, create_positions_frame, 
        create_round_trips_frame)
from ..analytics.stats import create_report

if TYPE_CHECKING:
    import pandas as pd
    from blueshift.core.utils.environment import TradingEnvironment
else:
    import blueshift.lib.common.lazy_pandas as pd

MINIMUM_UPDATE_TIME=1.5 # update requests below this will be refused

class PaperBlotter(IBlotter):
    """
        Blotter tracks the orders generated by the algo and matches them
        from the order status received from the broker API. It also computes
        the positions that should arise out of those algo orders and matches
        against the positions from broker API. Cumulative sum of the realized
        and unrealized pnls from these positions are algo pnl. This helps us
        avoid computing algo performance solely based on account information,
        as the account can also be affected by other means (manual trades or
        capital withdrawals etc.).

        Args:
            ``name (str)``: Unique name of the run.
            
            ``asset_finder (object)``: Asset finder object for the run.
            
            ``data_portal (object)``: Data portal object for the run.

            ``broker (object)``: Broker object for the run.

            ``env (object)``: trading environment.

            ``logger (object)``: Logger object of the run.

            ``init_capital (float)``: Ignored.

            ``timestamp (Timestamp)``: Timestamp at creation.
            
            ``env (object)``: trading environment.

            ``ccy (object)``: Blotter currency.
            
            ``output_dir (str)``: Path to output directory.
            
    """
    _FREQ = 60000000000
    
    def __init__(self, name:str, asset_finder:IAssetFinder, data_portal:DataPortal, broker:PaperBroker, 
                 env:TradingEnvironment, logger:logging.Logger|None=None, init_capital:float|None=None, 
                 timestamp:pd.Timestamp|None=None, ccy:Currency=CCY.LOCAL, output_dir=None): # type: ignore
        IBlotter.__init__(self, name)
        self._initialized = False
        self.ROUND_TRIPS_FILE = "round_trips.json"
        self.OPEN_TRADES_FILE = "open_trades.json"
        
        self._mode = AlgoMode.PAPER
        self.blotter_type = BlotterType.EXCLUSIVE
        self._ccy = ccy
        self._timestamp = timestamp
        self._daily_positions ={}
        self._matched_orders = {}
        
        if not output_dir:
            self._output_dir = blueshift_saved_performance_path(name)
        else:
            self._output_dir = output_dir
        ensure_directory(self._output_dir)
        
        self._asset_finder = asset_finder
        self._data_portal = data_portal
        self._broker = broker
        
        self._env = env
        if not self._env:
            msg = f'Failed to create blotter, no trading environment '
            msg += f'supplied, nor found.'
            raise InitializationError(msg)
        
        if logger:
            self.logger = logger
        else:
            self.logger = self._env.logger
        
        #self.trading_calendar = broker.calendar
        self.trading_calendar = cast(TradingCalendar, self._env.trading_calendar)
        tz=self.trading_calendar.tz
        self._perfs_tracker = PerformanceTracker(name, self._mode, tz=tz)
        self._last_updated = None
        self._last_reconciled = None
        self._internal_recon_counter = 0
        self._last_saved = None
        self._read_success = True
        
        if not self._env.alert_manager:
            msg = f'No alert manager defined for the trading environment.'
            raise InitializationError(msg)
            
        self.publisher = self._env.alert_manager.publisher
        self._lock = TimeoutRLock(timeout=LOCK_TIMEOUT)
                
        self.set_benchmark()
        self._record_vars = pd.DataFrame()
        self._initialized = True
            
    def _prefix_fn(self, timestamp, suffix='.json', dt_format="%Y%m%d"):
        return timestamp.date().strftime(dt_format) + suffix
    
    @property
    def name(self):
        return self._name
    
    @property
    def mode(self):
        return self._mode
    
    @property
    def ccy(self):
        return self._broker.ccy
    
    @property
    def asset_finder(self):
        return self._asset_finder
    
    @property
    def data_portal(self):
        return self._data_portal
    
    @property
    def broker(self):
        return self._broker
    
    @property
    def performance_tracker(self):
        return self._perfs_tracker
        
    @property
    def timestamp(self):
        return self._timestamp
    
    @timestamp.setter
    def timestamp(self, timestamp):
        self._timestamp = timestamp
        for name in self.blotters:
            self.blotters[name].timestamp = timestamp
        
    @property
    def account(self):
        self.reconcile(timestamp=self._timestamp, internal=True)
        with self._lock:
            return self._broker.get_account()
        
    @property
    def orders(self):
        self.reconcile(timestamp=self._timestamp, internal=True)
        with self._lock:
            return self._broker.orders
    
    def _orders_no_reconcile(self):
        with self._lock:
            return self._broker.orders
        
    @property
    def open_orders(self):
        self.reconcile(timestamp=self._timestamp, internal=True)
        with self._lock:
            return self._broker.open_orders
        
    @property
    def portfolio(self):
        self.reconcile(timestamp=self._timestamp, internal=True)
        with self._lock:
            return self._broker.positions.copy()
        
    @property
    def current_positions(self):
        self.reconcile(timestamp=self._timestamp, internal=True)
        with self._lock:
            return self._broker.positions
    
    @property
    def round_trips(self):
        self.reconcile(timestamp=self._timestamp, internal=True)
        with self._lock:
            return self._broker.round_trips
        
    @property
    def performance(self):
        self.reconcile(timestamp=self._timestamp, internal=True)
        return self._perfs_tracker.current_performance.copy()
        
    @property
    def perfs_history(self):
        self.reconcile(timestamp=self._timestamp, internal=True)
        perfs = self._perfs_tracker.to_dataframe()
        perfs.index = pd.to_datetime(perfs.index.astype('int64'))
        perfs.index = perfs.index.tz_localize('UTC').\
                            tz_convert(self.trading_calendar.tz)
        return perfs
        
    @property
    def current_performance(self):
        self.reconcile(timestamp=self._timestamp, internal=True)
        return self._perfs_tracker.current_performance
        
    @property
    def risk_report(self):
        return self._perfs_tracker.create_eod_report(False)
        
    @property
    def pnls(self):
        self.reconcile(timestamp=self._timestamp, internal=True)
        return self._perfs_tracker.to_dataframe_performance()
        
    @property
    def transactions(self):
        self.reconcile(timestamp=self._timestamp, internal=True)
        with self._lock:
            return self._broker.transactions
    
    @property
    def positions(self):
        return self._daily_positions
    
    def reset(self, timestamp=None, account=None, initial_positions=None, **kwargs) -> None:
        """
            Reset the blotter. this resets the transaction tracker as well.

            Args:
                ``account_net (float)``: To set the account value to.
                ``initial_positions (dict)``: A dict of positions as starting point.

            Returns:
                None.
        """
        if not initial_positions:
            initial_positions = {}
        self.broker.reset(open_positions=initial_positions)
        self._perfs_tracker.reset(self.account)
        self.sanitize()
    
    def set_record_vars(self, record_vars):
        """ set the recorded vars from context. """
        self._record_vars = record_vars
    
    def _save(self, timestamp=None, *args, **kwargs):
        if not self._read_success:
            if self.logger:
                msg = f'Skipping blotter data save as read was not successful.'
                self.logger.warning(msg)
                
        if self._last_saved and self._last_saved == timestamp:
            return
        
        perfs = pd.DataFrame()
        try:
            perfs = self._perfs_tracker._save(timestamp)
        except Exception as e:
            if isinstance(e, TerminationError):
                raise
                
            if self.logger:
                exc_type, exc_value, exc_traceback = sys.exc_info()
                err_msg = get_exception(exc_type, exc_value, exc_traceback)
                self.logger.info(err_msg)
                
            msg = f"failed to save performance data in blotter:{str(e)}"
            handling = ExceptionHandling.WARN
            raise DataWriteException(msg=msg, handling=handling)
        
        txns_df = pd.DataFrame()
        try:
            txns_df = create_txns_frame(
                    self.transactions, tz=self.trading_calendar.tz)
            if not txns_df.empty:
                fname = 'transactions.csv'
                fname = os_path.join(self._output_dir, fname)
                txns_df.to_csv(fname)
        except Exception as e:
            if isinstance(e, TerminationError):
                raise
                
            if self.logger:
                exc_type, exc_value, exc_traceback = sys.exc_info()
                err_msg = get_exception(exc_type, exc_value, exc_traceback)
                self.logger.info(err_msg)
                
            msg = f"failed to process transaction data in blotter:{str(e)}"
            handling = ExceptionHandling.WARN
            raise DataWriteException(msg=msg, handling=handling)
            
        pos_df = pd.DataFrame()
        try:
            pos_df = create_positions_frame(self.positions)
            if not pos_df.empty:
                fname = 'positions.csv'
                fname = os_path.join(self._output_dir, fname)
                pos_df.to_csv(fname)
        except (TypeError, KeyError, BlueshiftException) as e:
            if self.logger:
                exc_type, exc_value, exc_traceback = sys.exc_info()
                err_msg = get_exception(exc_type, exc_value, exc_traceback)
                self.logger.info(err_msg)
                
            msg = f"failed to process positions data in blotter:{str(e)}"
            handling = ExceptionHandling.WARN
            raise DataWriteException(msg=msg, handling=handling)
            
        trip_df = pd.DataFrame()
        try:
            fname = 'round_trips.csv'
            if self.round_trips:
                trip_df = create_round_trips_frame(
                        self.round_trips, perfs.net)
                if not trip_df.empty:
                    fname = os_path.join(self._output_dir, fname)
                    trip_df.to_csv(fname)
        except (TypeError, OSError) as e:
            if self.logger:
                exc_type, exc_value, exc_traceback = sys.exc_info()
                err_msg = get_exception(exc_type, exc_value, exc_traceback)
                self.logger.info(err_msg)
                
            msg = f"failed to process round trips data in blotter:{str(e)}"    
            handling = ExceptionHandling.WARN
            raise DataWriteException(msg=msg, handling=handling)
        
        round_trips = pd.DataFrame()
        report = {}
        try:
            if not perfs.empty:
                report, round_trips = create_report(
                        perfs, round_trips=trip_df)
            if not round_trips.empty:
                fname = 'pertrade.csv'
                round_trips.to_csv(os_path.join(self._output_dir, fname))
            if report:
                fname = 'stats.json'
                fname = os_path.join(self._output_dir, fname)
                with open(fname, 'w') as fp:
                    convert_nan(report)
                    json.dump(report, fp, cls=BlueshiftJSONEncoder)
        except (TypeError, OSError) as e:
            if self.logger:
                exc_type, exc_value, exc_traceback = sys.exc_info()
                err_msg = get_exception(exc_type, exc_value, exc_traceback)
                self.logger.info(err_msg)
                
            msg = f"failed to process stats data in blotter:{str(e)}"
            handling = ExceptionHandling.WARN
            raise DataWriteException(msg=msg, handling=handling)
            
        try:
            fname = 'record_vars.csv'
            if not self._record_vars.empty:
                self._record_vars.to_csv(
                        os_path.join(self._output_dir, fname))
        except (TypeError, OSError) as e:
            if self.logger:
                exc_type, exc_value, exc_traceback = sys.exc_info()
                err_msg = get_exception(exc_type, exc_value, exc_traceback)
                self.logger.info(err_msg)
                
            msg = f"failed to process record vars data in blotter:{str(e)}"
            handling = ExceptionHandling.WARN
            raise DataWriteException(msg=msg, handling=handling)
            
        # skip saving charts and htmls
            
        try:
            target = os_path.join(blueshift_run_path(self.name), 'simulator')
            ensure_directory(target)
            self.broker.save(target)
        except Exception as e:
            if self.logger:
                exc_type, exc_value, exc_traceback = sys.exc_info()
                err_msg = get_exception(exc_type, exc_value, exc_traceback)
                self.logger.info(err_msg)
                
            msg = f"failed to save simulator data:{str(e)}."
            raise DataWriteException(msg=msg)
            
        if self.logger:
            self.logger.info(
                    f'successfully saved blotter data at {timestamp}.')
            
        self._last_saved = timestamp
        if self.logger:
            self.logger.info(
                    f'successfully saved blotter data at {timestamp}.')
    
    def _read(self, timestamp=None, *args, **kwargs):
        """
            Read/ reload the blotter.
        """
        self._read_success = False
        try:
            target = os_path.join(blueshift_run_path(self.name), 'simulator')
            if not os_path.exists(target):
                self._read_success = True
                return
            self.broker.read(target)
        except Exception as e:
            if self.logger:
                exc_type, exc_value, exc_traceback = sys.exc_info()
                err_msg = get_exception(exc_type, exc_value, exc_traceback)
                self.logger.info(err_msg)
                
            msg = f"failed to read simulator data:{str(e)}."
            raise DataWriteException(msg=msg)
            
        try:
            self._perfs_tracker._read(timestamp)
        except Exception as e:
            if self.logger:
                exc_type, exc_value, exc_traceback = sys.exc_info()
                err_msg = get_exception(exc_type, exc_value, exc_traceback)
                self.logger.info(err_msg)
                
            msg = f"failed to read saved performance data:{str(e)}."
            raise DataWriteException(msg=msg)
            
        self._read_success = True
        if self.logger:
            self.logger.info(
                    f'successfully read saved blotter data at {timestamp}.')
            
    def sanitize(self):
        pos = self.portfolio
        
        # asset expiry date are without tz-info
        today = pd.Timestamp.now(
                tz=self.trading_calendar.tz).normalize().tz_localize(
                        None)
        for asset in pos:
            try:
                self.asset_finder.symbol(asset.exchange_ticker, logger=self.logger)
            except Exception:
                msg = f'Cannot restart with unknown asset {asset} in'
                msg += f' open position.'
                raise AlgoStartError(msg)
                
            if hasattr(asset, 'expiry_date') and asset.expiry_date < today: # type: ignore
                msg = f'Cannot restart with expired asset {asset} in open position.'
                raise AlgoStartError(msg)
                
        
        if self._broker.open_orders:
            # any open orders will not be saved in next run, any recon will fail
            msg = f'Cannot restart with open orders.'
            raise AlgoStartError(msg)
    
    def _roll(self, timestamp, *args, **kwargs):
        try:
            self._daily_positions[timestamp] = {}
            current_positions = self.portfolio
            for asset in current_positions:
                pos = current_positions[asset]
                self._daily_positions[timestamp][asset.exchange_ticker]=pos.to_json()
                
            self._perfs_tracker.update_metrics(
                    self._broker.get_account(), self._broker.orders, 
                    self._broker.positions.copy(), timestamp.value)
            self._perfs_tracker.roll(timestamp)
            self._last_updated = timestamp
            self._last_reconciled = timestamp
        except Exception as e:
            if isinstance(e, TerminationError):
                raise

            if self.logger:
                exc_type, exc_value, exc_traceback = sys.exc_info()
                err_msg = get_exception(exc_type, exc_value, exc_traceback)
                self.logger.info(err_msg)

            msg = f'failed to roll the blotter on {timestamp}, see logs for details: {str(e)}'
            raise BlueshiftException(msg=msg)
    
    def _finalize(self, timestamp, *args, **kwargs):
        exit_time = kwargs.pop('exit_time', False)
        if exit_time or self._last_updated is None or \
            self._last_updated and self._last_updated != timestamp:
            # update only if we are quitting before a roll
            try:
                self._perfs_tracker.update_metrics(
                        self._broker.get_account(), self._broker.orders, 
                        self._broker.positions.copy(), timestamp.value)
                self._last_updated = timestamp
            except Exception as e:
                if isinstance(e, TerminationError):
                    raise

                if self.logger:
                    exc_type, exc_value, exc_traceback = sys.exc_info()
                    err_msg = get_exception(exc_type, exc_value, exc_traceback)
                    self.logger.info(err_msg)
                
                msg = f'failed to update performance tracker, see logs for details: {str(e)}'
                raise BlueshiftException(msg=msg)
        
        try:
            self.update_perf_report(timestamp)
        except Exception as e:
            msg = f'failed to create performance report: {str(e)}'
            raise BlueshiftException(msg=msg)
        
        self._save(timestamp)
        
    def set_benchmark(self, benchmark=None):
        """
            Set up benchmark handling. Benchmark can be either a symbol 
            available in the daily data portal, or a csv file location or a 
            pandas Series object. The benchmark related calculations are 
            triggered daily and input benchmark data (if not a symbol) is 
            assumed to have daily frequency.
        """
        if not benchmark:
            benchmark = self._env.benchmark
                
        if benchmark is None:
            try:
                benchmark = self._broker.library.get_benchmark(
                        self._env.start_dt, self._env.end_dt)
            except Exception:
                self._benchmark = None
                #self._benchmark_rets = None
                msg = 'No benchmark specified. Paper trading will run without benchmark.'
                msg = msg + ' Some metrics related to benchmark will be missing.'
                self.logger.warn(msg)
                return
        elif isinstance(benchmark, str):
            if benchmark.endswith('.csv'):
                benchmark = pd.read_csv(benchmark, index_col=0)
                benchmark.index = pd.DatetimeIndex(
                        pd.to_datetime(benchmark.index))
                if benchmark.index.tz: # type: ignore
                    benchmark.index = benchmark.index.tz_convert( # type: ignore
                            self.trading_calendar.tz)
                else:
                    benchmark.index = benchmark.index.tz_localize( # type: ignore
                            self.trading_calendar.tz)
                benchmark.index = benchmark.index.normalize() # type: ignore
                benchmark = benchmark.iloc[:,0]
            else:
                if not self._env:
                    msg = 'Environment not found, cannot compute benchmark.'
                    self.logger.warn(msg)
                    return
                start_dt = self._env.start_dt
                end_dt = self._env.end_dt
                start_dt = cast(pd.Timestamp, start_dt)
                end_dt = cast(pd.Timestamp, end_dt)
                asset = self._broker.library.symbol(benchmark)
                benchmark = self._broker.library.read_between(
                        asset,'close',start_dt, end_dt, frequency='1d')
        elif isinstance(benchmark, MarketData):
            if not self._env:
                msg = 'Environment not found, cannot compute benchmark.'
                self.logger.warn(msg)
                return
            start_dt = self._env.start_dt
            end_dt = self._env.end_dt
            start_dt = cast(pd.Timestamp, start_dt)
            end_dt = cast(pd.Timestamp, end_dt)
            asset = self._broker.library.symbol(benchmark.symbol)
            benchmark = self._broker.library.read_between(
                    asset,'close',start_dt, end_dt, frequency='1d')
            
        if not isinstance(benchmark, pd.Series):
            msg = f'benchmark must be a csv file location, a valid symbol'
            msg = msg + f' or a pandas series with date-time index.'
            raise InitializationError(msg)
        
        if benchmark.empty:
            msg = 'No data for benchmark. Paper trading will run without benchmark.'
            msg = msg + ' Some metrics related to benchmark will be missing.'
            self.logger.warn(msg)
            return
        
        if isinstance(benchmark.index, pd.DatetimeIndex):
            if benchmark.index.tz:
                benchmark.index = benchmark.index.tz_convert(self._broker.tz)
            else:
                benchmark.index = benchmark.index.tz_localize(
                        'Etc/UTC').tz_convert(self._broker.tz)
        
        self._perfs_tracker.set_benchmark(benchmark)
        
    def update_benchmark(self, timestamp=None, benchmark=None):
        """
            For backtest, this method does not apply as we fetch the 
            whole benchmark data during the `set_benchmark` call.
        """
        pass
    
    def update_perf_report(self, timestamp):
        """
            Compute host of analytics. For backtest usually this will be
            called at the end of the algo run. For live mode, this is
            called at end-of-day. This includes a per-trade analytics pack,
            a transaction reports pack and a risk statistics pack.
        """
        pass
    
    def check_on_restart(self, *args, **kwargs):
        """
            Do validation, if any, on an algo restart.
        """
        self.sanitize()
        
    def reconcile(self, timestamp, internal=False, *args, **kwargs) -> bool:
        """
            Reconcile algo transactions, positions and accounts with 
            broker values.
        """
        if not self._env.algo or not self._initialized:
            return True
        
        if not self._env.algo.is_TRADING_BAR():
            return True
        
        
        try:
            with self._lock:
                if internal:
                    self._internal_recon_counter += 1
                else:
                    self._internal_recon_counter = 0
                    
                if self._internal_recon_counter > 2:
                    # hard protection against unintended recursion
                    self._internal_recon_counter = 0
                    return True
                
                if not timestamp:
                    timestamp = pd.Timestamp.now(tz=self.trading_calendar.tz)
                    
                forced = kwargs.get('forced', False)
                pending_check = False
                if self.needs_reconciliation or not self._last_reconciled:
                    pending_check = True
                
                ts_check = False
                if timestamp and self._last_reconciled:
                    # a minimum floor of MINIMUM_UPDATE_TIME
                    if abs((timestamp - self._last_reconciled).total_seconds()) < MINIMUM_UPDATE_TIME \
                        and not forced:
                        return True
                    
                    if self._last_reconciled < self._normalize_ts(timestamp):
                        ts_check = True
                    
                can_skip = not (pending_check or ts_check or forced)
                if can_skip:
                    return True
                
            
                if timestamp:
                    self._broker.paper_reconcile(timestamp)
                    self._perfs_tracker.update_metrics(
                                self._broker.get_account(), self._broker.orders,
                                self._broker.positions.copy(), timestamp.value)
                
                matched_ids = list(self._matched_orders.keys())
                matched_orders = {}
                broker_orders = self._broker.orders
                for oid in matched_ids:
                    if oid not in broker_orders:
                        self.logger.error(f'missing order ID {oid} in reconciliation.')
                    elif broker_orders[oid].is_final():
                        matched_orders[oid] = broker_orders[oid]
                        self._matched_orders.pop(oid, None)
                        
                open_orders = self._broker.open_orders.copy()
                if open_orders:
                    self.emit_transactions_msg(open_orders)
                if matched_orders:
                    self.emit_transactions_msg(matched_orders)
                    
                self._last_reconciled = timestamp
                self._needs_reconciliation = False
                return True
        except Exception as e:
            if isinstance(e, TerminationError):
                raise

            if self.logger:
                exc_type, exc_value, exc_traceback = sys.exc_info()
                err_msg = get_exception(exc_type, exc_value, exc_traceback)
                self.logger.info(err_msg)

            self.logger.error(f'reconciliation could not be done, see logs for details: {str(e)}.')
            return False
    
    def _update_valuation(self, timestamp, timeout=None):
        if not self._env.algo:
            return True
        
        if not self._env.algo.is_TRADING_BAR():
            return True
        
        if not timestamp:
            timestamp = self.timestamp
        orig_ts = timestamp
        
        try:
            with self._lock:
                pending_check = False
                if self.needs_reconciliation or not self._last_updated:
                    pending_check = True
                
                ts_check = False
                if timestamp and self._last_updated:
                    # also put a minimum floor of MINIMUM_UPDATE_TIME
                    if abs((timestamp - self._last_updated).total_seconds()) < MINIMUM_UPDATE_TIME:
                        return True
                    
                    if self._last_updated < self._normalize_ts(timestamp):
                        ts_check = True
                    
                can_skip = not (pending_check or ts_check)
                if can_skip:
                    return True
            
                if timestamp:
                    self._broker.paper_reconcile(timestamp)
                    self._perfs_tracker.update_metrics(
                                self._broker.get_account(), self._broker.orders, 
                                self._broker.positions.copy(), timestamp.value)
                    
                self._last_updated = orig_ts
                self._needs_reconciliation = False
        except Exception as e:
            if isinstance(e, TerminationError):
                raise

            if self.logger:
                exc_type, exc_value, exc_traceback = sys.exc_info()
                err_msg = get_exception(exc_type, exc_value, exc_traceback)
                self.logger.info(err_msg)

            self.logger.info(f'valuation update failed, see logs for details: {str(e)}.')
            return False
    
    def add_transactions(self, order_id, order, commissions, charges, **kwargs):
        """
            Add a transactions for future reconciliation.
        """
        self._matched_orders[order_id] = order
        self.emit_transactions_msg({order.oid:order})
        
    def simulate(self, timestamp):
        with self._lock:
            self.broker.paper_simulate(timestamp)
        
        for name in self.blotters:
            self.blotters[name].simulate(timestamp=timestamp)
            
        if AlgoCallBack.TRADE in self.broker.events and self._env.algo:
            self.broker.events.pop(AlgoCallBack.TRADE)
            self._env.algo.schedule_event(AlgoCallBack.TRADE, param=None)
            
        if AlgoCallBack.RECONCILIATION in self.broker.events and self._env.algo:
            oids = list(self.broker.events.pop(AlgoCallBack.RECONCILIATION, []))
            self._env.algo.schedule_event(AlgoCallBack.RECONCILIATION, param=oids)
        
        
    def add_blotter(self, strategy, timestamp=None):
        """ 
            Add a botter.
        """
        broker = self._env.create_broker_copy(strategy.initial_capital)
        broker = cast(PaperBroker, broker)
        
        output_dir = None
        logger = self.logger
        output_dir = os_path.join(self._output_dir, strategy.name)
        
        blotter = PaperBlotter(
                strategy.name, 
                self.asset_finder,
                self.data_portal, 
                broker=broker,
                logger=logger, 
                init_capital=strategy.initial_capital, 
                timestamp=timestamp, 
                env=self._env, 
                ccy=self.ccy,
                output_dir=output_dir)
        
        self.blotters[blotter.name] = blotter
        blotter.topic = self.name
        blotter.parent = self
        
        return blotter

register_blotter('paper', PaperBlotter)